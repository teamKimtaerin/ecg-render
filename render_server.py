"""
GPU Render Server for MotionText Video Processing
EC2 GPU ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë Œë”ë§ ì„œë²„
"""

import sys
from pathlib import Path
import logging
import os
import warnings
from typing import Dict, Any, Optional
from datetime import datetime
import asyncio
import uuid

from dotenv import load_dotenv
load_dotenv()

warnings.filterwarnings("ignore", category=UserWarning)

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
import uvicorn

from modules.queue import RenderQueue, RenderJob
from modules.worker import RenderWorker
from modules.parallel_worker import ParallelRenderWorker
from src.logger import get_logger

# Error code constants
class ErrorCodes:
    GPU_MEMORY_INSUFFICIENT = "GPU_MEMORY_INSUFFICIENT"
    INVALID_VIDEO_FORMAT = "INVALID_VIDEO_FORMAT"
    SCENARIO_PARSE_ERROR = "SCENARIO_PARSE_ERROR"
    RENDERING_TIMEOUT = "RENDERING_TIMEOUT"
    STORAGE_ACCESS_ERROR = "STORAGE_ACCESS_ERROR"
    CONNECTION_ERROR = "CONNECTION_ERROR"
    TIMEOUT_ERROR = "TIMEOUT_ERROR"
    RENDER_ERROR = "RENDER_ERROR"
    CANCELLED = "CANCELLED"

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="GPU Render Server",
    description="GPU ê¸°ë°˜ MotionText ë¹„ë””ì˜¤ ë Œë”ë§ ì„œë²„",
    version="1.0.0",
)

# í™˜ê²½ ì„¤ì •
S3_BUCKET = os.getenv("S3_BUCKET", "ecg-rendered-videos")
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
BACKEND_CALLBACK_URL = os.getenv("BACKEND_CALLBACK_URL", "")
MAX_CONCURRENT_JOBS = int(os.getenv("MAX_CONCURRENT_JOBS", "3"))
TEMP_DIR = os.getenv("TEMP_DIR", "/tmp/render")
CALLBACK_RETRY_COUNT = int(os.getenv("CALLBACK_RETRY_COUNT", "3"))
CALLBACK_TIMEOUT = int(os.getenv("CALLBACK_TIMEOUT", "30"))
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

# Parallel rendering configuration
USE_PARALLEL_RENDERING = os.getenv("USE_PARALLEL_RENDERING", "true").lower() == "true"
BROWSER_POOL_SIZE = int(os.getenv("BROWSER_POOL_SIZE", "4"))

# Render mode configuration (celery or standalone)
RENDER_MODE = os.getenv("RENDER_MODE", "standalone")  # "celery" or "standalone"

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

# ì „ì—­ ê°ì²´
render_queue: Optional[RenderQueue] = None
render_workers: list[RenderWorker] = []
worker_tasks: list[asyncio.Task] = []


# ========== Pydantic Models ==========

class RenderRequest(BaseModel):
    """ë Œë”ë§ ìš”ì²­ ëª¨ë¸"""
    jobId: str
    videoUrl: str
    scenario: Dict[str, Any]  # MotionText ì‹œë‚˜ë¦¬ì˜¤
    options: Dict[str, Any] = {
        "width": 1920,
        "height": 1080,
        "fps": 30,
        "quality": 90,
        "format": "mp4"
    }
    callbackUrl: str


class RenderResponse(BaseModel):
    """ë Œë”ë§ ì‘ë‹µ ëª¨ë¸"""
    status: str
    job_id: str
    message: str
    estimated_time: Optional[int] = None


class JobStatus(BaseModel):
    """ì‘ì—… ìƒíƒœ ëª¨ë¸"""
    job_id: str
    status: str
    progress: int
    message: Optional[str] = None
    download_url: Optional[str] = None
    error_message: Optional[str] = None
    error_code: Optional[str] = None
    estimated_time_remaining: Optional[int] = None


# ========== Startup/Shutdown Events ==========

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ì´ˆê¸°í™”"""
    global render_queue, render_workers, worker_tasks

    logger.info("GPU ë Œë”ë§ ì„œë²„ ì´ˆê¸°í™” ì¤‘...")

    # Redis í ì´ˆê¸°í™”
    render_queue = RenderQueue(redis_url=REDIS_URL)

    # ì›Œì»¤ ì„¤ì •
    worker_config = {
        "s3_bucket": S3_BUCKET,
        "temp_dir": TEMP_DIR,
    }

    # ì›Œì»¤ ìƒì„± ë° ì‹œì‘
    if USE_PARALLEL_RENDERING:
        # Use parallel workers for better performance
        logger.info(f"ğŸš€ Using PARALLEL rendering with {BROWSER_POOL_SIZE} browser instances per worker")

        for i in range(MAX_CONCURRENT_JOBS):
            worker = ParallelRenderWorker(
                render_queue,
                worker_config,
                pool_size=BROWSER_POOL_SIZE
            )
            render_workers.append(worker)

            # ë¹„ë™ê¸° íƒœìŠ¤í¬ë¡œ ì›Œì»¤ ì‹œì‘
            task = asyncio.create_task(worker.start())
            worker_tasks.append(task)

            logger.info(f"ë³‘ë ¬ ë Œë”ë§ ì›Œì»¤ {i+1} ì‹œì‘ë¨ (pool size: {BROWSER_POOL_SIZE})")
    else:
        # Use legacy sequential workers
        logger.info("Using legacy sequential rendering")

        for i in range(MAX_CONCURRENT_JOBS):
            worker = RenderWorker(render_queue, worker_config)
            render_workers.append(worker)

            # ë¹„ë™ê¸° íƒœìŠ¤í¬ë¡œ ì›Œì»¤ ì‹œì‘
            task = asyncio.create_task(worker.start())
            worker_tasks.append(task)

            logger.info(f"ë Œë”ë§ ì›Œì»¤ {i+1} ì‹œì‘ë¨")

    # GPU ìƒíƒœ í™•ì¸
    check_gpu_status()

    if USE_PARALLEL_RENDERING:
        total_browsers = MAX_CONCURRENT_JOBS * BROWSER_POOL_SIZE
        logger.info(f"âœ… GPU ë Œë”ë§ ì„œë²„ ì¤€ë¹„ ì™„ë£Œ (ì›Œì»¤: {MAX_CONCURRENT_JOBS}ê°œ, ì´ ë¸Œë¼ìš°ì €: {total_browsers}ê°œ)")
    else:
        logger.info(f"âœ… GPU ë Œë”ë§ ì„œë²„ ì¤€ë¹„ ì™„ë£Œ (ì›Œì»¤: {MAX_CONCURRENT_JOBS}ê°œ)")


@app.on_event("shutdown")
async def shutdown_event():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
    global render_workers, worker_tasks

    logger.info("GPU ë Œë”ë§ ì„œë²„ ì¢…ë£Œ ì¤‘...")

    # ëª¨ë“  ì›Œì»¤ ì •ì§€
    for worker in render_workers:
        await worker.stop()

    # ì›Œì»¤ íƒœìŠ¤í¬ ì·¨ì†Œ
    for task in worker_tasks:
        task.cancel()
        try:
            await task
        except asyncio.CancelledError:
            pass

    logger.info("âœ… GPU ë Œë”ë§ ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")


# ========== Helper Functions ==========

def check_gpu_status():
    """GPU ìƒíƒœ í™•ì¸"""
    try:
        import torch

        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            logger.info(f"ğŸ® GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_count}ê°œ")

            for i in range(gpu_count):
                props = torch.cuda.get_device_properties(i)
                logger.info(f"  GPU {i}: {props.name}")
                logger.info(f"    - ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.1f} GB")
                logger.info(f"    - CUDA ì½”ì–´: {props.multi_processor_count}")
        else:
            logger.warning("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    except ImportError:
        logger.warning("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # FFmpeg GPU ì¸ì½”ë”© í™•ì¸
    try:
        import subprocess
        result = subprocess.run(
            ['ffmpeg', '-hide_banner', '-encoders'],
            capture_output=True,
            text=True,
            timeout=5
        )

        if 'h264_nvenc' in result.stdout:
            logger.info("âœ… NVENC GPU ì¸ì½”ë”© ì‚¬ìš© ê°€ëŠ¥")
        else:
            logger.warning("âš ï¸ NVENC GPU ì¸ì½”ë”©ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"FFmpeg í™•ì¸ ì‹¤íŒ¨: {e}")


# ========== API Endpoints ==========

@app.post("/render", response_model=RenderResponse)
async def process_render_job(request: RenderRequest):
    """ë Œë”ë§ ì‘ì—… ìˆ˜ì‹  ë° íì— ì¶”ê°€"""
    try:
        logger.info(f"ë Œë”ë§ ìš”ì²­ ìˆ˜ì‹  - job_id: {request.jobId}")

        # RenderJob ìƒì„±
        job = RenderJob(
            job_id=request.jobId,
            video_url=request.videoUrl,
            scenario=request.scenario,
            options=request.options,
            callback_url=request.callbackUrl
        )

        # íì— ì¶”ê°€
        await render_queue.add_job(job)

        logger.info(f"ì‘ì—…ì´ íì— ì¶”ê°€ë¨ - job_id: {request.jobId}")

        return RenderResponse(
            status="accepted",
            job_id=request.jobId,
            message="Job queued for processing",
            estimated_time=180  # ê¸°ë³¸ ì˜ˆìƒ ì‹œê°„ 3ë¶„
        )

    except Exception as e:
        logger.error(f"ì‘ì—… ì¶”ê°€ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to queue job: {str(e)}"
        )


@app.post("/api/render/{job_id}/cancel")
async def cancel_render_job(job_id: str):
    """ë Œë”ë§ ì‘ì—… ì·¨ì†Œ"""
    try:
        success = render_queue.cancel_job(job_id)

        if success:
            logger.info(f"ì‘ì—… ì·¨ì†Œë¨ - job_id: {job_id}")
            return {"success": True, "message": "Job cancelled successfully"}
        else:
            return {"success": False, "message": "Job not found or already completed"}

    except Exception as e:
        logger.error(f"ì‘ì—… ì·¨ì†Œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to cancel job: {str(e)}"
        )


@app.get("/api/render/{job_id}/status", response_model=JobStatus)
async def get_job_status(job_id: str):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    try:
        job = render_queue.get_job(job_id)

        if not job:
            raise HTTPException(
                status_code=404,
                detail="Job not found"
            )

        return JobStatus(
            job_id=job.job_id,
            status=job.status,
            progress=job.progress,
            message=f"Job is {job.status}",
            error_message=job.error_message
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get job status: {str(e)}"
        )


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        queue_status = render_queue.get_queue_status() if render_queue else {}

        # GPU ë©”ëª¨ë¦¬ ì²´í¬
        gpu_info = {}
        try:
            import torch
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    reserved = torch.cuda.memory_reserved(i) / 1024**3
                    gpu_info[f"gpu_{i}"] = {
                        "allocated_gb": round(allocated, 2),
                        "reserved_gb": round(reserved, 2)
                    }
        except:
            pass

        return {
            "status": "healthy",
            "gpu_count": len(gpu_info) if gpu_info else 0,
            "available_memory": f"{sum([24 - info.get('reserved_gb', 0) for info in gpu_info.values()])}GB" if gpu_info else "Unknown",
            "queue_length": queue_status.get('pending', 0) if queue_status else 0,
            "timestamp": datetime.now().isoformat(),
            "queue": queue_status,
            "gpu": gpu_info,
            "workers": len(render_workers),
            "parallel_rendering": USE_PARALLEL_RENDERING,
            "browser_pool_size": BROWSER_POOL_SIZE if USE_PARALLEL_RENDERING else 0
        }

    except Exception as e:
        logger.error(f"í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


@app.get("/queue/status")
async def get_queue_status():
    """í ìƒíƒœ ì¡°íšŒ"""
    try:
        status = render_queue.get_queue_status()
        # ì˜ˆìƒ ëŒ€ê¸° ì‹œê°„ ê³„ì‚° (ê´€ì˜ˆìƒ ì‘ì—…ë³„ 3ë¶„)
        pending_jobs = status.get('pending', 0)
        processing_jobs = status.get('processing', 0)
        estimated_wait_time = pending_jobs * 180  # 3ë¶„ per job

        return {
            "pending_jobs": pending_jobs,
            "processing_jobs": processing_jobs,
            "estimated_wait_time": estimated_wait_time,
            "detailed_status": status
        }

    except Exception as e:
        logger.error(f"í ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get queue status: {str(e)}"
        )


@app.post("/api/render/queue/clear")
async def clear_queue():
    """í ì´ˆê¸°í™” (í…ŒìŠ¤íŠ¸ìš©)"""
    try:
        if os.getenv("ENVIRONMENT") != "production":
            render_queue.clear_queue()
            return {"success": True, "message": "Queue cleared"}
        else:
            raise HTTPException(
                status_code=403,
                detail="Queue clearing is disabled in production"
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"í ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to clear queue: {str(e)}"
        )


# ========== Main ==========

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="GPU Render Server")
    parser.add_argument("--host", default="0.0.0.0", help="ë°”ì¸ë“œ í˜¸ìŠ¤íŠ¸")
    parser.add_argument("--port", type=int, default=8090, help="ë°”ì¸ë“œ í¬íŠ¸")
    parser.add_argument("--workers", type=int, default=1, help="ìœ ë¹„ì½˜ ì›Œì»¤ ìˆ˜")
    parser.add_argument(
        "--log-level", default="info",
        choices=["debug", "info", "warning", "error"]
    )
    parser.add_argument(
        "--mode", default=RENDER_MODE,
        choices=["standalone", "celery"],
        help="ì‹¤í–‰ ëª¨ë“œ: standalone (ë…ë¦½ ì„œë²„) ë˜ëŠ” celery (ì›Œì»¤)"
    )

    args = parser.parse_args()

    # Override render mode if specified
    if args.mode:
        RENDER_MODE = args.mode

    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=getattr(logging, args.log_level.upper()),
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    )

    logger.info("ğŸš€ GPU ë Œë”ë§ ì„œë²„ ì‹œì‘")
    logger.info(f"   ì‹¤í–‰ ëª¨ë“œ: {RENDER_MODE}")
    logger.info(f"   í˜¸ìŠ¤íŠ¸: {args.host}:{args.port}")
    logger.info(f"   Redis: {REDIS_URL}")
    logger.info(f"   S3 ë²„í‚·: {S3_BUCKET}")

    if RENDER_MODE == "celery":
        # Celery worker mode
        logger.info("ğŸ”§ Celery Worker ëª¨ë“œë¡œ ì‹¤í–‰")
        logger.info("   Celery workerë¥¼ ì‹œì‘í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:")
        logger.info("   python celery_worker.py")
        logger.info("")
        logger.info("   ë˜ëŠ” Docker Composeë¡œ ì‹¤í–‰:")
        logger.info("   docker-compose -f docker-compose.celery.yml up -d")

        # Import and run celery worker
        import subprocess
        import sys

        celery_cmd = [
            sys.executable,
            "celery_worker.py"
        ]

        logger.info(f"Celery worker ì‹œì‘ ì¤‘...")
        subprocess.run(celery_cmd)

    else:
        # Standalone server mode
        logger.info("ğŸ–¥ï¸ Standalone ì„œë²„ ëª¨ë“œë¡œ ì‹¤í–‰")
        logger.info(f"   ìœ ë¹„ì½˜ ì›Œì»¤: {args.workers}")
        logger.info(f"   ë Œë”ë§ ì›Œì»¤: {MAX_CONCURRENT_JOBS}")
        logger.info(f"   ì½œë°± URL: {BACKEND_CALLBACK_URL if BACKEND_CALLBACK_URL else 'Not configured'}")
        logger.info(f"   ë³‘ë ¬ ë Œë”ë§: {'í™œì„±í™”' if USE_PARALLEL_RENDERING else 'ë¹„í™œì„±í™”'}")
        if USE_PARALLEL_RENDERING:
            logger.info(f"   ë¸Œë¼ìš°ì € í’€ í¬ê¸°: {BROWSER_POOL_SIZE}ê°œ/ì›Œì»¤")

        # FastAPI ì„œë²„ ì‹¤í–‰
        uvicorn.run(
            "render_server:app",
            host=args.host,
            port=args.port,
            workers=args.workers,
            access_log=True,
            log_level=args.log_level,
            reload=False
        )